{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/trl.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up imports and environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kensh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\kensh\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "import huggingface_hub\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments,pipeline,logging\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "huggingface_hub.login(token = \"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\", write_permission =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data):\n",
    "    return f\"\"\"\n",
    "            Analyze the open price enclosed in the first square bracket, dayily high enclosed in the second\n",
    "            square bracket, daily low in the third square bracket, close price in the fourth square barcket,\n",
    "            volume on the fifth square bracket, and sentiments in the sixth square bracket, determine whether or not\n",
    "            the sentiment embedded have a positive effect on the market price, a\n",
    "            nd return the corresponding label of \"Yes\" or \"No\"\n",
    "\n",
    "            [{data[\"open\"]}] [{data[\"high\"]}] [{data[\"low\"]}] [{data[\"close\"]}] [{data[\"volume\"]}] [{data[\"sentiment_nltk\"]}]={data[\"up\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data):\n",
    "    return f\"\"\"\n",
    "             Analyze the open price enclosed in the first square bracket, dayily high enclosed in the second\n",
    "            square bracket, daily low in the third square bracket, close price in the fourth square barcket,\n",
    "            volume on the fifth square bracket, and sentiments in the sixth square bracket, determine whether or not\n",
    "            the sentiment embedded have a positive effect on the market price, a\n",
    "            nd return the corresponding label of \"Yes\" or \"No\"\n",
    "\n",
    "            [{data[\"open\"]}] [{data[\"high\"]}] [{data[\"low\"]}] [{data[\"close\"]}] [{data[\"volume\"]}] [{data[\"sentiment_nltk\"]}]=\n",
    "            \"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file  = \"../yahoo_news_preprocessed.csv\"\n",
    "\n",
    "df = pd.read_csv(file, names = [\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"sentiment_nltk\",\"up\"], encoding = \"utf-8\", encoding_errors = \"replace\")\n",
    "df[\"sentiment_nltk\"] = df[\"sentiment_nltk\"].replace({\"1\":\"Positive\",\"-1\":\"Negative\",\"0\":\"Neutural\"})\n",
    "df[\"up\"] = df[\"up\"].replace({\"0\":\"Yes\",\"1\":\"No\"})\n",
    "df[\"index\"] = df.index\n",
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "# positive_count = (df[\"up\"] == \"Yes\").sum()\n",
    "# neg_count = len(df) - positive_count\n",
    "# print(positive_count)\n",
    "# print(neg_count)\n",
    "\n",
    "for result in [\"Yes\", \"No\"]:\n",
    "    train, test = train_test_split(df[df.up == result], train_size = 300, test_size=200, random_state=42)\n",
    "    x_train.append(train)\n",
    "    x_test.append(test)\n",
    "\n",
    "x_train = pd.concat(x_train).sample(frac = 1)\n",
    "x_test = pd.concat(x_test)\n",
    "eval_index = [indx for indx in df.index if indx not in list(x_train.index) + list(x_test.index)]\n",
    "x_eval = df[df.index.isin(eval_index)]\n",
    "x_eval = x_eval.groupby(\"up\",group_keys=False).apply(lambda x: x.sample(n=100,replace=True))\n",
    "x_train = x_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "x_train_prompts = pd.DataFrame(x_train.apply(generate_prompt, axis=1), columns=['prompt'])\n",
    "x_eval_prompts = pd.DataFrame(x_eval.apply(generate_prompt, axis=1), columns=['prompt'])\n",
    "\n",
    "y_test = x_test.up\n",
    "x_test_prompts = pd.DataFrame(x_test.apply(generate_test_prompt, axis=1), columns=['prompt'])\n",
    "\n",
    "train_data = Dataset.from_pandas(x_train_prompts)\n",
    "eval_data = Dataset.from_pandas(x_eval_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutaion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    mapping = {\"Yes\":0,\"No\":1}\n",
    "    y_true = np.vectorize(lambda x: mapping.get(x,1))(y_true)\n",
    "    y_pred = np.vectorize(lambda x: mapping.get(x,1))(y_pred)\n",
    "\n",
    "    accuracy = accuracy_score(y_true = y_true, y_pred = y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    unique_labels = set(y_pred)\n",
    "    for label in unique_labels:\n",
    "        label_indicies = [i for i in range(len(y_true)) if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indicies]\n",
    "        label_y_pred = [y_pred[i] for i in label_indicies]\n",
    "        accuracy = accuracy_score(label_y_true,label_y_pred)\n",
    "        print(f\"Accuracy for Label {label}: {accuracy:.3f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\\n{classification_report(y_true = y_true, y_pred=y_pred)}\")\n",
    "    print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_true = y_true, y_pred = y_pred, labels = [0,1])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1764a3b1ca0244a4844a1cd541b10e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-7b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"float16\")\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test_prompts, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(x_test_prompts))):\n",
    "        prompt = x_test_prompts.iloc[i][\"prompt\"]\n",
    "        input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = model.generate(**input, max_new_tokens = 1, temperature = 0.0)\n",
    "        \n",
    "        result = tokenizer.decode(output[0])\n",
    "        answer = result.split(\"=\")[-1]\n",
    "        y_pred.append(answer)\n",
    "       \n",
    "       \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model Before fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:51<00:00,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.470\n",
      "Accuracy for Label 0: 0.805\n",
      "Accuracy for Label 1: 0.135\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.81      0.60       200\n",
      "           1       0.41      0.14      0.20       200\n",
      "\n",
      "    accuracy                           0.47       400\n",
      "   macro avg       0.45      0.47      0.40       400\n",
      "weighted avg       0.45      0.47      0.40       400\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[161  39]\n",
      " [173  27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(x_test_prompts,model,tokenizer)\n",
    "evaluate(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target module Identity() is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m      1\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m      2\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-linear\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:227\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[0;32m    223\u001b[0m             output\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    225\u001b[0m         model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mregister_forward_hook(make_inputs_require_grad)\n\u001b[1;32m--> 227\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    229\u001b[0m     args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbf16\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded_qlora\n\u001b[0;32m    233\u001b[0m ):\n\u001b[0;32m    234\u001b[0m     peft_module_casting_to_bf16(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\mapping.py:136\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    135\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\peft_model.py:1094\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1094\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\peft_model.py:129\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:136\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[1;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:148\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:325\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name)\u001b[0m\n\u001b[0;32m    323\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    324\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:220\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[1;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[0;32m    210\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[0;32m    211\u001b[0m         adapter_name,\n\u001b[0;32m    212\u001b[0m         r,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         use_dora\u001b[38;5;241m=\u001b[39mlora_config\u001b[38;5;241m.\u001b[39muse_dora,\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter:\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[0;32m    223\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:301\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[1;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;66;03m# no module could be matched\u001b[39;00m\n\u001b[1;32m--> 301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. Currently, only the following modules are supported: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m     )\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "\u001b[1;31mValueError\u001b[0m: Target module Identity() is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"logs\",\n",
    "    num_train_epochs=5,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 112,\n",
    "    eval_accumulation_steps=1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
